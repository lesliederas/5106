# -*- coding: utf-8 -*-
"""Hw1Part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aXvOAwbF5VO7gMeYtfL_J640Bo_vPjea
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load your dataset
data = pd.read_csv('Housing.csv')

# Assuming columns 'area', 'bedrooms', 'bathrooms', 'stories', 'parking'' are the features and 'Price' is the target
X = data[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']].values
y = data['price'].values

# Split data into 80% training and 20% validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Build the multi-layer perceptron model
model = Sequential([
    Dense(64, activation='relu', input_dim=X_train.shape[1]),
    Dense(32, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=1)

# Plot training & validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
print()
# Evaluate the model
y_pred = model.predict(X_val)
mse = mean_squared_error(y_val, y_pred)
print(f"Final Validation MSE: {mse}")

"""2.b"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load your dataset
data = pd.read_csv('Housing.csv')

# Assuming 'stories' or 'parking' is a categorical feature, and 'area', 'bedrooms', 'bathrooms' are numerical
categorical_columns = ['stories', 'parking']  # Adjust according to your dataset
numerical_columns = ['area', 'bedrooms', 'bathrooms']  # Adjust according to your dataset

# Define feature matrix and target variable
X = data[categorical_columns + numerical_columns]
y = data['price'].values

# Split data into 80% training and 20% validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the preprocessing pipeline (OneHotEncoder for categorical features and StandardScaler for numerical features)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_columns),
        ('cat', OneHotEncoder(), categorical_columns)
    ])

# Preprocess the data (transform it using the ColumnTransformer)
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_val_preprocessed = preprocessor.transform(X_val)

# Build the multi-layer perceptron model
model = Sequential([
    Dense(64, activation='relu', input_dim=X_train_preprocessed.shape[1]),
    Dense(32, activation='relu'),
    Dense(1)
])

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history = model.fit(X_train_preprocessed, y_train, epochs=100, batch_size=32, validation_data=(X_val_preprocessed, y_val), verbose=1)

# Plot training & validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluate the model
y_pred = model.predict(X_val_preprocessed)
mse = mean_squared_error(y_val, y_pred)
print(f"Final Validation MSE: {mse}")

# Print model complexity (number of trainable parameters)
model.summary()

"""2.c"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Load your dataset
data = pd.read_csv('Housing.csv')

# Assuming 'stories' or 'parking' is a categorical feature, and 'area', 'bedrooms', 'bathrooms' are numerical
categorical_columns = ['stories', 'parking']  # Adjust according to your dataset
numerical_columns = ['area', 'bedrooms', 'bathrooms']  # Adjust according to your dataset

# Define feature matrix and target variable
X = data[categorical_columns + numerical_columns]
y = data['price'].values

# Split data into 80% training and 20% validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the preprocessing pipeline (OneHotEncoder for categorical features and StandardScaler for numerical features)
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_columns),
        ('cat', OneHotEncoder(), categorical_columns)
    ])

# Preprocess the data (transform it using the ColumnTransformer)
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_val_preprocessed = preprocessor.transform(X_val)

# Build the more complex multi-layer perceptron model
model_complex = Sequential([
    Dense(128, activation='relu', input_dim=X_train_preprocessed.shape[1]),  # Increase the number of units
    Dense(64, activation='relu'),  # Add more layers
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),  # Additional layer
    Dense(1)  # Output layer for regression
])

# Compile the model
model_complex.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
history_complex = model_complex.fit(X_train_preprocessed, y_train, epochs=100, batch_size=32, validation_data=(X_val_preprocessed, y_val), verbose=1)

# Plot training & validation loss
plt.plot(history_complex.history['loss'], label='Train Loss - Complex')
plt.plot(history_complex.history['val_loss'], label='Validation Loss - Complex')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Evaluate the model
y_pred_complex = model_complex.predict(X_val_preprocessed)
mse_complex = mean_squared_error(y_val, y_pred_complex)
print(f"Final Validation MSE (Complex Model): {mse_complex}")

# Print model complexity (number of trainable parameters)
model_complex.summary()